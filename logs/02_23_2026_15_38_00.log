{"timestamp": "2026-02-23T10:08:24.203818Z", "level": "info", "event": "Initializing ApiKeyManager"}
{"timestamp": "2026-02-23T10:08:24.204797Z", "level": "info", "event": "GOOGLE_API_KEY loaded successfully from environment"}
{"timestamp": "2026-02-23T10:08:24.204797Z", "level": "info", "event": "GROQ_API_KEY loaded successfully from environment"}
{"path": "F:\\Sharan AI Projects\\GenAI AgenticAI Projects\\Automated_research_report_generation\\research_and_analyst\\config\\configuration.yaml", "keys": ["astra_db", "embedding_model", "retriever", "llm"], "timestamp": "2026-02-23T10:08:24.207569Z", "level": "info", "event": "Configuration loaded successfully"}
{"config_keys": ["astra_db", "embedding_model", "retriever", "llm"], "timestamp": "2026-02-23T10:08:24.208467Z", "level": "info", "event": "YAML configuration loaded successfully"}
{"provider": "groq", "model": "deepseek-r1-distill-llama-70b", "timestamp": "2026-02-23T10:08:24.209515Z", "level": "info", "event": "Loading LLM"}
{"provider": "groq", "model": "deepseek-r1-distill-llama-70b", "timestamp": "2026-02-23T10:08:24.572657Z", "level": "info", "event": "LLM loaded successfully"}
{"module": "AutonomousReportGenerator", "timestamp": "2026-02-23T10:08:24.573167Z", "level": "info", "event": "Building report generation graph"}
{"module": "InterviewGraphBuilder", "timestamp": "2026-02-23T10:08:24.575928Z", "level": "info", "event": "Building Interview Graph workflow"}
{"module": "InterviewGraphBuilder", "timestamp": "2026-02-23T10:08:24.593233Z", "level": "info", "event": "Interview Graph compiled successfully"}
{"module": "AutonomousReportGenerator", "timestamp": "2026-02-23T10:08:24.610891Z", "level": "info", "event": "Report generation graph built successfully"}
{"module": "ReportService", "topic": "GenAI in Civil Engineering", "thread_id": "de53574e-7790-4567-901b-18910cbf0752", "timestamp": "2026-02-23T10:08:24.610891Z", "level": "info", "event": "Starting report pipeline"}
{"module": "AutonomousReportGenerator", "topic": "GenAI in Civil Engineering", "timestamp": "2026-02-23T10:08:24.618474Z", "level": "info", "event": "Creating analyst personas"}
HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
{"module": "AutonomousReportGenerator", "error": "Error code: 400 - {'error': {'message': 'The model `deepseek-r1-distill-llama-70b` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}", "timestamp": "2026-02-23T10:08:24.843101Z", "level": "error", "event": "Error creating analysts"}
{"module": "ReportService", "error": "Error in [F:\\Sharan AI Projects\\GenAI AgenticAI Projects\\Automated_research_report_generation\\env\\Lib\\site-packages\\groq\\_base_client.py] at line [1044] | Message: Failed to create analysts\nTraceback:\nTraceback (most recent call last):\n  File \"F:\\Sharan AI Projects\\GenAI AgenticAI Projects\\Automated_research_report_generation\\research_and_analyst\\workflows\\report_generator_workflow.py\", line 66, in create_analyst\n    analysts = invoke_with_retry(structured_llm, [\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Sharan AI Projects\\GenAI AgenticAI Projects\\Automated_research_report_generation\\research_and_analyst\\utils\\rate_limiter.py\", line 56, in invoke_with_retry\n    return llm.invoke(messages, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Sharan AI Projects\\GenAI AgenticAI Projects\\Automated_research_report_generation\\env\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3244, in invoke\n    input_ = context.run(step.invoke, input_, config, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Sharan AI Projects\\GenAI AgenticAI Projects\\Automated_research_report_generation\\env\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5711, in invoke\n    return self.bound.invoke(\n           ^^^^^^^^^^^^^^^^^^\n  File \"F:\\Sharan AI Projects\\GenAI AgenticAI Projects\\Automated_research_report_generation\\env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 395, in invoke\n    self.generate_prompt(\n  File \"F:\\Sharan AI Projects\\GenAI AgenticAI Projects\\Automated_research_report_generation\\env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1025, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Sharan AI Projects\\GenAI AgenticAI Projects\\Automated_research_report_generation\\env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 842, in generate\n    self._generate_with_cache(\n  File \"F:\\Sharan AI Projects\\GenAI AgenticAI Projects\\Automated_research_report_generation\\env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1091, in _generate_with_cache\n    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"F:\\Sharan AI Projects\\GenAI AgenticAI Projects\\Automated_research_report_generation\\env\\Lib\\site-packages\\langchain_groq\\chat_models.py\", line 557, in _generate\n    response = self.client.create(messages=message_dicts, **params)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Sharan AI Projects\\GenAI AgenticAI Projects\\Automated_research_report_generation\\env\\Lib\\site-packages\\groq\\resources\\chat\\completions.py\", line 461, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"F:\\Sharan AI Projects\\GenAI AgenticAI Projects\\Automated_research_report_generation\\env\\Lib\\site-packages\\groq\\_base_client.py\", line 1242, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Sharan AI Projects\\GenAI AgenticAI Projects\\Automated_research_report_generation\\env\\Lib\\site-packages\\groq\\_base_client.py\", line 1044, in request\n    raise self._make_status_error_from_response(err.response) from None\ngroq.BadRequestError: Error code: 400 - {'error': {'message': 'The model `deepseek-r1-distill-llama-70b` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n", "timestamp": "2026-02-23T10:08:24.852622Z", "level": "error", "event": "Error initiating report generation"}
